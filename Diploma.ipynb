{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Дополнительные установки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Работа с табличными данными\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение моделей, кодирование категориальных признаков\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Распараллеливание операций с большими данными\n",
    "!pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели Word2Vec для эмбеддингов\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка датасета\n",
    "!pip install kagglehub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание и обучение нейронных сетей, работа с тензорами\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавление прогресс-бара в цикл\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оптимизация гиперпараметров модели \n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQ4AWsEhT4qj"
   },
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "path = kagglehub.dataset_download(\"retailrocket/ecommerce-dataset\")\n",
    "events = pd.read_csv(path+'/events.csv')\n",
    "item_properties_p1 = pd.read_csv(path+'/item_properties_part1.csv')\n",
    "item_properties_p2 = pd.read_csv(path+'/item_properties_part2.csv')\n",
    "item_properties = pd.concat([item_properties_p1, item_properties_p2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bxXtdMAOUH-w"
   },
   "outputs": [],
   "source": [
    "print(events.head(5))\n",
    "print(item_properties.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNRoLxUOWSOw"
   },
   "outputs": [],
   "source": [
    "events.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BLOx3T4Wtn6"
   },
   "outputs": [],
   "source": [
    "item_properties.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5ygRAiI5xiu"
   },
   "outputs": [],
   "source": [
    "# Очистка данных (дубли/null)\n",
    "events = events.drop_duplicates()\n",
    "item_properties = item_properties.drop_duplicates()\n",
    "events = events.dropna(subset = ['event'])\n",
    "events = events.dropna(subset = ['itemid'])\n",
    "events = events.dropna(subset = ['visitorid'])\n",
    "events = events.dropna(subset = ['timestamp'])\n",
    "item_properties = item_properties.dropna(subset = ['itemid'])\n",
    "item_properties = item_properties.dropna(subset = ['timestamp'])\n",
    "\n",
    "# Приведение timestamp к единому типу данных для merge\n",
    "events['timestamp'] = events['timestamp'].astype('int64')\n",
    "item_properties['timestamp'] = item_properties['timestamp'].astype('int64')\n",
    "events = events.sort_values(['timestamp']).reset_index(drop=True)\n",
    "item_properties = item_properties.sort_values(['timestamp']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Соединение events и item_properties\n",
    "merged_data = pd.merge_asof(\n",
    "    events,\n",
    "    item_properties,\n",
    "    on='timestamp',\n",
    "    by='itemid',\n",
    "    direction='backward'\n",
    ")\n",
    "# Проверка что все правильно соединилось без дублей\n",
    "print(\"Было:\", len(events), \",стало:\", len(merged_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание сессий (действия пользователя в течение 30 минут)\n",
    "merged_data = merged_data.drop(['transactionid'], axis = 1)\n",
    "merged_data['property'] = merged_data['property'].fillna(0)\n",
    "merged_data['value'] = merged_data['value'].fillna(0)\n",
    "merged_data['timestamp'] = pd.to_datetime(merged_data['timestamp'], unit='ms')\n",
    "merged_data = merged_data.sort_values(by=['visitorid', 'timestamp'])\n",
    "\n",
    "merged_data['session_id'] = (\n",
    "    (merged_data['timestamp'].diff() >= pd.Timedelta(minutes=30)) | \n",
    "    (merged_data['visitorid'] != merged_data['visitorid'].shift())\n",
    ").cumsum()\n",
    "\n",
    "print(merged_data.head(5))\n",
    "print(\"Уникальных сессий:\", merged_data['session_id'].nunique())\n",
    "print(\"Уникальных пользователей:\", merged_data['visitorid'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание эмбеддингов property/value\n",
    "merged_data['property_value'] = merged_data['property'].astype(str) + ':' + merged_data['value'].astype(str)\n",
    "property_value_texts = merged_data.groupby('itemid')['property_value'].apply(lambda x: ' '.join(x)).values\n",
    "property_value_tokens = [text.split() for text in property_value_texts]\n",
    "property_value_model = Word2Vec(sentences=property_value_tokens, vector_size=16, window=5, min_count=1, sg=1, epochs=10)\n",
    "property_value_embeddings = {word: property_value_model.wv[word] for word in property_value_model.wv.index_to_key}\n",
    "\n",
    "def get_item_embedding(item_properties):\n",
    "    embeddings = [property_value_embeddings[prop] for prop in item_properties if prop in property_value_embeddings]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(32)\n",
    "\n",
    "item_embeddings = {itemid: get_item_embedding(text.split()) for itemid, text in zip(merged_data['itemid'].unique(), property_value_texts)}\n",
    "item_embeddings_df = pd.DataFrame.from_dict(item_embeddings, orient='index')\n",
    "item_embeddings_df.reset_index(inplace=True)\n",
    "item_embeddings_df.rename(columns={'index': 'itemid'}, inplace=True)\n",
    "embedding_columns = list(item_embeddings_df.columns)\n",
    "\n",
    "# Присоединение эмбеддингов к основной таблице\n",
    "merged_data = merged_data.merge(item_embeddings_df, on='itemid', how='left')\n",
    "\n",
    "# Создание эмбеддингов event\n",
    "event_encoder = LabelEncoder()\n",
    "merged_data['event_encoded'] = event_encoder.fit_transform(merged_data['event'])\n",
    "print(merged_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_data = dd.from_pandas(merged_data, npartitions=100)\n",
    "# Папка для .npz файлов\n",
    "os.makedirs(\"files_npz\", exist_ok=True)\n",
    "\n",
    "def create_sequence_for_session(session_df):\n",
    "    \"\"\"\n",
    "    Функция для создания последовательностей признаков и меток для сессии.\n",
    "    Принимает DataFrame с данными одной сессии и возвращает X (последовательности признаков)\n",
    "    и y (метки событий).\n",
    "\n",
    "    Параметры:\n",
    "    session_df (pd.DataFrame): DataFrame, содержащий данные для одной сессии.\n",
    "\n",
    "    Возвращает:\n",
    "    tuple или None: Возвращает кортеж (X, y) с последовательностями признаков и метками,\n",
    "    если последовательности были успешно созданы, иначе возвращает None.\n",
    "    \"\"\"\n",
    "    session_df = session_df.sort_values('timestamp')\n",
    "    features = session_df.iloc[:, 10:].values \n",
    "    events = session_df['event_encoded'].values.reshape(-1, 1)\n",
    "    X, y = [], []\n",
    "    seq_length = 10\n",
    "    for i in range(len(features) - seq_length):\n",
    "        X.append(features[i:i + seq_length])\n",
    "        y.append(events[i + seq_length])      \n",
    "    if len(X) > 0:\n",
    "        return np.array(X), np.array(y)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df = dask_data.compute()\n",
    "groups = list(df.groupby(['visitorid', 'session_id']))\n",
    "\n",
    "# Цикл по всем группам 'visitorid' + 'session_id' и сохранение данных в .npz файлы\n",
    "for (visitor_id, session_id), group in tqdm(groups, desc=\"Saving .npz files\"):\n",
    "    if len(group) >= 10:\n",
    "        result = create_sequence_for_session(group)\n",
    "        if result is not None:\n",
    "            X, y = result\n",
    "            filename = f\"session_{visitor_id}_{session_id}.npz\"\n",
    "            filepath = os.path.join(\"files_npz\", filename)\n",
    "            np.savez_compressed(filepath, X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка что dask-файлы создались\n",
    "directory = os.path.join(os.getcwd(), \"files_npz\")\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".npz\"):\n",
    "            print(\"Найден файл:\", os.path.join(root, file))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные из dask-файлов -> признаки и метки для обучения\n",
    "def load_data(directory):\n",
    "    X, y = [], []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".npz\"):\n",
    "            data = np.load(os.path.join(directory, file))\n",
    "            X.append(data['X'])\n",
    "            y.append(data['y'])\n",
    "    return np.concatenate(X, axis=0), np.concatenate(y, axis=0)\n",
    "\n",
    "directory = os.path.join(os.getcwd(), \"files_npz\")\n",
    "X, y = load_data(directory)\n",
    "                               \n",
    "# Проверка на корректный тип данных и размерности\n",
    "print(f\"Тип X: {type(X)}\")\n",
    "print(f\"Тип y: {type(y)}\")\n",
    "print(f\"Размер X: {X.shape}\")\n",
    "print(f\"Размер y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Приведение последовательностей к одинаковой длине\n",
    "seq_length = 10\n",
    "\n",
    "def custom_pad_sequences(X, seq_length):\n",
    "    padded_sequences = []\n",
    "    \n",
    "    for seq in X:\n",
    "        if len(seq) < seq_length:\n",
    "            padding = np.zeros((seq_length - len(seq), seq.shape[1]))\n",
    "            padded_seq = np.vstack([seq, padding])\n",
    "        else:\n",
    "            padded_seq = seq[:seq_length]\n",
    "        padded_sequences.append(padded_seq)\n",
    "    \n",
    "    return np.array(padded_sequences)\n",
    "\n",
    "X_padded = custom_pad_sequences(X, seq_length)\n",
    "print(f\"X_padded.shape: {X_padded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_padded,\n",
    "                y,stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Размер X_train: {X_train.shape}\")\n",
    "print(f\"Размер X_test: {X_test.shape}\")\n",
    "print(f\"Размер y_train: {y_train.shape}\")\n",
    "print(f\"Размер y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель LRCN (базовая)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование данных в тензоры для использования в модели\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).squeeze()\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).squeeze()\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Класс модели\n",
    "Наследуется от nn.Module (базовый класс для всех моделей в PyTorch)\n",
    "\"\"\"\n",
    "class LRCN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LRCN, self).__init__()\n",
    "        \n",
    "        # CNN часть\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # LSTM часть\n",
    "        self.lstm = nn.LSTM(input_size=256, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, seq_length, input_size) -> (batch_size, input_size, seq_length)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x) \n",
    "        x = x.permute(0, 2, 1)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        output = self.fc(lstm_out)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Функция для обучения модели\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Эпоха [{epoch+1}/{num_epochs}], Потеря: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Функция для оценки модели на тестовых данных\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * (sum(1 for p, l in zip(all_preds, all_labels) if p == l) / len(all_labels))\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объявление параметров модели\n",
    "input_size = X_train.shape[2]\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = len(np.unique(y))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Объявление модели\n",
    "lrcn_model = LRCN(input_size, hidden_size, num_layers, num_classes)\n",
    "optimizer = torch.optim.Adam(lrcn_model.parameters(), lr=0.001)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lrcn_model.to(device)\n",
    "# Обучение и оценка модели\n",
    "start_time = time.time()\n",
    "train_model(lrcn_model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "evaluate_model(lrcn_model, test_loader)\n",
    "end_time = time.time()\n",
    "print(f\"Время работы: {end_time - start_time:.2f} секунд\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшенная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование данных в тензоры\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).squeeze()\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).squeeze()\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Деление train_dataset на train и val\n",
    "val_size = int(0.2 * len(train_dataset))\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Класс модели\n",
    "Наследуется от nn.Module (базовый класс для всех моделей в PyTorch)\n",
    "\"\"\"\n",
    "class ImprovedLRCN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3):\n",
    "        super(ImprovedLRCN, self).__init__()\n",
    "\n",
    "        # CNN часть\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.norm1 = nn.LayerNorm(256)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # LSTM часть\n",
    "        self.lstm = nn.LSTM(input_size=256, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.attention = nn.Linear(hidden_size * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.gelu(self.conv1(x))\n",
    "        x = self.gelu(self.conv2(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "        output = self.fc(context_vector)\n",
    "        return output\n",
    "\n",
    "# Функция для обучения модели\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        scheduler.step(total_loss / len(train_loader))\n",
    "        print(f\"Эпоха [{epoch + 1}/{num_epochs}], Потеря: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Функция для оценки модели на тестовых данных\n",
    "def evaluate_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Функция для подбора гиперпараметров -> увеличения эффективности\n",
    "def objective(trial):\n",
    "    hidden_size = trial.suggest_int('hidden_size', 64, 256)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "\n",
    "    model = ImprovedLRCN(input_size=X_train.shape[2], hidden_size=hidden_size, num_layers=num_layers,\n",
    "                         num_classes=len(np.unique(y)), dropout=dropout).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "    train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=5)\n",
    "    _, _, _, f1 = evaluate_model(model, val_loader, criterion)\n",
    "\n",
    "    trial.report(f1, 0)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WARNING: работает очень долго (~1 час)\n",
    "\"\"\"\n",
    "# Подбор гиперпараметров \n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "best_params = study.best_params\n",
    "print(f\"Лучшие параметры: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объявление параметров модели \n",
    "hidden_size = 125\n",
    "num_layers = 3\n",
    "dropout = 0.36597\n",
    "lr = 0.0006\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Объявление модели\n",
    "improved_lrcn = ImprovedLRCN(input_size=X_train.shape[2], \n",
    "                    hidden_size=hidden_size, num_layers=num_layers,\n",
    "                   num_classes=len(np.unique(y)), dropout=dropout)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "improved_lrcn.to(device)\n",
    "optimizer = optim.AdamW(improved_lrcn.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "# Обучение и оценка модели\n",
    "start_time = time.time()\n",
    "train_model(improved_lrcn, train_loader, criterion, optimizer, \n",
    "            scheduler, num_epochs=10)\n",
    "accuracy, precision, recall, f1 = evaluate_model(improved_lrcn, val_loader,\n",
    "                                                 criterion)\n",
    "end_time = time.time()\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"Время работы: {end_time - start_time:.2f} секунд\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнение с другими моделями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feedforward neural networks\n",
    "- Recurrent neural networks with LSTM cells\n",
    "- Transformer-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FFNN ожидает один фиксированный вектор признаков на каждый пример,\n",
    "признаки с разными масштабами усложняют процесс обучения\n",
    "Усреднение → превращаем последовательность в один вектор признаков\n",
    "Стандартизация → делаем признаки \"одинаковыми\" для лучшего обучения\n",
    "\"\"\"\n",
    "X_train_ffnn = np.mean(X_train, axis=1)\n",
    "X_test_ffnn = np.mean(X_test, axis=1)\n",
    "scaler = StandardScaler()\n",
    "X_train_ffnn = scaler.fit_transform(X_train_ffnn)\n",
    "X_test_ffnn = scaler.transform(X_test_ffnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование данных в тензоры для использования в модели\n",
    "X_train_tensor = torch.tensor(X_train_ffnn, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_ffnn, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).squeeze()\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).squeeze()\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Класс модели\n",
    "Наследуется от nn.Module (базовый класс для всех моделей в PyTorch)\n",
    "\"\"\"\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Функция для обучения модели\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f\"Эпоха [{epoch+1}/{num_epochs}], Потеря: {avg_loss:.4f}\")\n",
    "\n",
    "# Функция для оценки модели на тестовых данных\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объявление параметров модели\n",
    "input_size = X_train_ffnn.shape[1] \n",
    "hidden_size = 128\n",
    "num_classes = len(np.unique(y))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Объявление модели\n",
    "ffnn_model = FFNN(input_size, hidden_size, num_classes)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ffnn_model.to(device)\n",
    "optimizer = optim.Adam(ffnn_model.parameters(), lr=0.001)\n",
    "# Обучение и оценка модели\n",
    "start_time = time.time()\n",
    "train_model(ffnn_model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "evaluate_model(ffnn_model, test_loader)\n",
    "end_time = time.time()\n",
    "print(f\"Время работы: {end_time - start_time:.2f} секунд\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование данных в тензоры для использования в модели\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).squeeze()\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).squeeze()\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Класс модели\n",
    "Наследуется от nn.Module (базовый класс для всех моделей в PyTorch)\n",
    "\"\"\"\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Используем последний выход LSTM\n",
    "        out = self.fc(lstm_out)\n",
    "        return out\n",
    "    \n",
    "# Функция для обучения модели    \n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Эпоха [{epoch + 1}/{num_epochs}], Потеря: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Функция для оценки модели на тестовых данных\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объявление параметров модели\n",
    "input_size = X_train.shape[2]\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = len(np.unique(y))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Объявление модели\n",
    "lstm_model = LSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lstm_model.to(device)\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "# Обучение и оценка модели\n",
    "start_time = time.time()\n",
    "train_model(lstm_model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "evaluate_model(lstm_model, test_loader)  \n",
    "end_time = time.time()\n",
    "print(f\"Время работы: {end_time - start_time:.2f} секунд\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование данных в тензоры для использования в модели\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).squeeze()\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).squeeze()\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Класс модели\n",
    "Наследуется от nn.Module (базовый класс для всех моделей в PyTorch)\n",
    "\"\"\"\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, feature_dim, d_model, nhead, num_layers, num_classes, dim_feedforward=128, dropout=0.1):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding = nn.Linear(feature_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # [batch_size, seq_len, d_model]\n",
    "        x = self.transformer_encoder(x)  # [batch_size, seq_len, d_model]\n",
    "        x = x.mean(dim=1)  # Average pooling over time dimension\n",
    "        out = self.classifier(x)  # [batch_size, num_classes]\n",
    "        return out\n",
    "\n",
    "# Функция для обучения модели\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Эпоха [{epoch + 1}/{num_epochs}], Потеря: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Функция для оценки модели на тестовых данных\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объявление параметров модели\n",
    "sequence_length = X_train.shape[1] \n",
    "feature_dim = X_train.shape[2]      \n",
    "num_classes = len(np.unique(y_train_tensor.numpy()))\n",
    "d_model = 64 \n",
    "nhead = 4\n",
    "num_layers = 2\n",
    "dim_feedforward = 128\n",
    "dropout = 0.1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Объявление модели\n",
    "transformer_model = TransformerClassifier(feature_dim=feature_dim,\n",
    "    d_model=d_model, nhead=nhead, num_layers=num_layers,\n",
    "    num_classes=num_classes, dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transformer_model.to(device)\n",
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.001)\n",
    "# Обучение и оценка модели\n",
    "start_time = time.time()\n",
    "train_model(transformer_model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "evaluate_model(transformer_model, test_loader)\n",
    "end_time = time.time()\n",
    "print(f\"Время работы: {end_time - start_time:.2f} секунд\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
