{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8b50018",
      "metadata": {
        "id": "c8b50018"
      },
      "outputs": [],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72210495",
      "metadata": {
        "id": "72210495"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec4d699b",
      "metadata": {
        "id": "ec4d699b"
      },
      "outputs": [],
      "source": [
        "!pip install dask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56ceeb5d",
      "metadata": {
        "id": "56ceeb5d"
      },
      "outputs": [],
      "source": [
        "!pip install kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21854116",
      "metadata": {
        "id": "21854116"
      },
      "outputs": [],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff76ed75",
      "metadata": {
        "id": "ff76ed75"
      },
      "source": [
        "# Предобработка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa762298",
      "metadata": {
        "id": "aa762298"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cdb153e",
      "metadata": {
        "id": "4cdb153e"
      },
      "outputs": [],
      "source": [
        "# Загрузка данных\n",
        "path = kagglehub.dataset_download(\"retailrocket/ecommerce-dataset\")\n",
        "events = pd.read_csv(path+'/events.csv')\n",
        "item_properties_p1 = pd.read_csv(path+'/item_properties_part1.csv')\n",
        "item_properties_p2 = pd.read_csv(path+'/item_properties_part2.csv')\n",
        "item_properties = pd.concat([item_properties_p1, item_properties_p2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecce0ba8",
      "metadata": {
        "id": "ecce0ba8"
      },
      "outputs": [],
      "source": [
        "# Очистка данных (дубли/null)\n",
        "events = events.drop_duplicates()\n",
        "item_properties = item_properties.drop_duplicates()\n",
        "events = events.dropna(subset = ['event'])\n",
        "events = events.dropna(subset = ['itemid'])\n",
        "events = events.dropna(subset = ['visitorid'])\n",
        "events = events.dropna(subset = ['timestamp'])\n",
        "item_properties = item_properties.dropna(subset = ['itemid'])\n",
        "item_properties = item_properties.dropna(subset = ['timestamp'])\n",
        "\n",
        "# Приведение timestamp к единому типу данных\n",
        "events['timestamp'] = events['timestamp'].astype('int64')\n",
        "item_properties['timestamp'] = item_properties['timestamp'].astype('int64')\n",
        "events = events.sort_values(['timestamp']).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "808810b0",
      "metadata": {
        "id": "808810b0"
      },
      "outputs": [],
      "source": [
        "item_properties.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de1a4373",
      "metadata": {
        "id": "de1a4373"
      },
      "outputs": [],
      "source": [
        "categories = item_properties[item_properties['property'] == 'categoryid']\n",
        "categories = categories.sort_values('timestamp').drop_duplicates('itemid', keep='first')\n",
        "categories = categories.rename(columns={'value': 'categoryid'})\n",
        "categories = categories.drop(['property'], axis=1)\n",
        "print(categories.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48c5b96b",
      "metadata": {
        "id": "48c5b96b"
      },
      "outputs": [],
      "source": [
        "# Соединение events и categories\n",
        "merged_data = pd.merge_asof(\n",
        "    events,\n",
        "    categories,\n",
        "    on='timestamp',\n",
        "    by='itemid',\n",
        "    direction='backward'\n",
        ")\n",
        "# Проверка что все правильно соединилось без дублей\n",
        "print(\"Было:\", len(events), \",стало:\", len(merged_data))\n",
        "merged_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "836aa031",
      "metadata": {
        "id": "836aa031"
      },
      "outputs": [],
      "source": [
        "merged_data = merged_data.dropna(subset=['categoryid'])\n",
        "print(\"Было:\", len(events), \",стало:\", len(merged_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4576ace",
      "metadata": {
        "id": "b4576ace"
      },
      "outputs": [],
      "source": [
        "# Создание сессий (действия пользователя в течение 30 минут)\n",
        "merged_data = merged_data.drop(['transactionid'], axis = 1)\n",
        "merged_data['timestamp'] = pd.to_datetime(merged_data['timestamp'], unit='ms')\n",
        "merged_data = merged_data.sort_values(by=['visitorid', 'timestamp'])\n",
        "\n",
        "merged_data['session_id'] = (\n",
        "    (merged_data['timestamp'].diff() >= pd.Timedelta(minutes=30)) |\n",
        "    (merged_data['visitorid'] != merged_data['visitorid'].shift())\n",
        ").cumsum()\n",
        "\n",
        "print(\"Уникальных сессий:\", merged_data['session_id'].nunique())\n",
        "print(\"Уникальных пользователей:\", merged_data['visitorid'].nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43f60a5f",
      "metadata": {
        "id": "43f60a5f"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "404f56c5",
      "metadata": {
        "id": "404f56c5"
      },
      "outputs": [],
      "source": [
        "label_encoder_category = LabelEncoder()\n",
        "merged_data['category_encoded'] = label_encoder_category.fit_transform(merged_data['categoryid'].astype(str))\n",
        "merged_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e181c34d",
      "metadata": {
        "id": "e181c34d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dask.dataframe as dd\n",
        "import dask.array as da\n",
        "from dask import delayed\n",
        "from dask.diagnostics import ProgressBar\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcf6fef4",
      "metadata": {
        "id": "fcf6fef4"
      },
      "outputs": [],
      "source": [
        "# Сборка Dask-DF\n",
        "\n",
        "ddf = dd.from_pandas(\n",
        "    merged_data[['session_id','category_encoded']],\n",
        "    npartitions=16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bed105a",
      "metadata": {
        "id": "0bed105a"
      },
      "outputs": [],
      "source": [
        "session_seqs = (\n",
        "    ddf\n",
        "      .groupby('session_id')['category_encoded']\n",
        "      .apply(lambda s: s.tolist(), meta=('list', object))\n",
        "      .compute()\n",
        ")\n",
        "# Оставляем только сессии длиной >= 5\n",
        "session_seqs = [seq for seq in session_seqs if len(seq) >= 5]\n",
        "\n",
        "# Генерация префиксов -> метки (следующая категория)\n",
        "sequences, labels = [], []\n",
        "for seq in session_seqs:\n",
        "    for i in range(1, len(seq)):\n",
        "        sequences.append(seq[:i])\n",
        "        labels.append(seq[i])\n",
        "\n",
        "# Усечение до фиксированной длины\n",
        "def pad_sequences(seqs, seq_length):\n",
        "    padded = np.zeros((len(seqs), seq_length), dtype=np.int64)\n",
        "    for i, seq in enumerate(seqs):\n",
        "        length = min(len(seq), seq_length)\n",
        "        padded[i, :length] = seq[:length]\n",
        "    return padded\n",
        "\n",
        "seq_length = 5\n",
        "X = pad_sequences(sequences, seq_length)\n",
        "y = np.array(labels, dtype=np.int64)\n",
        "\n",
        "# Фильтрация редких меток (<2 возникновений)\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "valid = unique[counts >= 2]\n",
        "mask = np.isin(y, valid)\n",
        "X, y = X[mask], y[mask]\n",
        "\n",
        "print(f\"После фильтрации: всего сэмплов = {len(y)}, \"\n",
        "      f\"уникальных классов = {len(np.unique(y))}\")\n",
        "\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.20,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val,\n",
        "    test_size=0.25,\n",
        "    random_state=42,\n",
        "    stratify=y_train_val\n",
        ")\n",
        "print(f\"Размер X_train: {X_train.shape}\")\n",
        "print(f\"Размер X_test:  {X_test.shape}\")\n",
        "print(f\"Размер X_val:  {X_val.shape}\")\n",
        "print(f\"Размер y_train: {y_train.shape}\")\n",
        "print(f\"Размер y_test: {y_test.shape}\")\n",
        "print(f\"Размер y_val:  {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a35b2075",
      "metadata": {
        "id": "a35b2075"
      },
      "source": [
        "# LRCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6045bae7",
      "metadata": {
        "id": "6045bae7"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import psutil\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split, WeightedRandomSampler\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b83aa47b",
      "metadata": {
        "id": "b83aa47b"
      },
      "source": [
        "## Классический LRCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fb26e16",
      "metadata": {
        "id": "6fb26e16"
      },
      "outputs": [],
      "source": [
        "def make_loader(X_arr, y_arr, shuffle=False):\n",
        "    # Преобразование данных в тензоры для использования в модели\n",
        "    ds = TensorDataset(\n",
        "        torch.tensor(X_arr, dtype=torch.long),\n",
        "        torch.tensor(y_arr, dtype=torch.long)\n",
        "    )\n",
        "    return DataLoader(ds, batch_size=64, shuffle=shuffle)\n",
        "\n",
        "# loader'ы для итераций по датасетам\n",
        "train_loader = make_loader(X_train, y_train, shuffle=True)\n",
        "val_loader   = make_loader(X_val,   y_val,   shuffle=False)\n",
        "test_loader  = make_loader(X_test,  y_test,  shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68ae2b3e",
      "metadata": {
        "id": "68ae2b3e"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Класс модели\n",
        "Наследуется от nn.Module (базовый класс для всех моделей в PyTorch)\n",
        "\"\"\"\n",
        "class LRCN(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_size, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        # embedding входных категорий\n",
        "        self.emb   = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        # CNN часть\n",
        "        self.conv1 = nn.Conv1d(in_channels=emb_dim, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.relu  = nn.ReLU()\n",
        "        self.pool  = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        # LSTM часть\n",
        "        self.lstm  = nn.LSTM(input_size=256, hidden_size=hidden_size,\n",
        "                             num_layers=num_layers, batch_first=True)\n",
        "        # классификатор\n",
        "        self.fc    = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        out, _ = self.lstm(x)\n",
        "        h = out[:, -1, :]\n",
        "        return self.fc(h)\n",
        "\n",
        "# Функция обучения\n",
        "def train_model(model, loader, criterion, optimizer, num_epochs=20):\n",
        "    model.train()\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        total_loss = 0\n",
        "        for Xb, yb in loader:\n",
        "            Xb, yb = Xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(Xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch}/{num_epochs} — loss={total_loss/len(loader):.4f}\")\n",
        "\n",
        "# Функция оценки\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in loader:\n",
        "            Xb = Xb.to(device)\n",
        "            logits = model(Xb)\n",
        "            preds = logits.argmax(dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(yb.numpy())\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    rec = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    return acc, prec, rec, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd83c103",
      "metadata": {
        "id": "cd83c103"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "vocab_size = int(X.max()) + 1\n",
        "num_classes = vocab_size\n",
        "\n",
        "# Замер текущей памяти\n",
        "process = psutil.Process(os.getpid())\n",
        "mem_before = process.memory_info().rss\n",
        "\n",
        "# Тренировка и оценка классической LRCN\n",
        "lrcn = LRCN(vocab_size, emb_dim=128, hidden_size=128, num_layers=2, num_classes=num_classes).to(device)\n",
        "opt = optim.Adam(lrcn.parameters(), lr=1e-3)\n",
        "crit = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "train_model(lrcn, train_loader, crit, opt, num_epochs=20)\n",
        "test_acc, precision, recall, f1 = evaluate_model(lrcn, test_loader)\n",
        "\n",
        "print(f\"Время работы: {time.time()-start:.1f}s\")\n",
        "\n",
        "# Память после\n",
        "mem_after = process.memory_info().rss\n",
        "mem_used = mem_after - mem_before\n",
        "\n",
        "print(f\"Потребление памяти процесса: {mem_used / (1024**2):.2f} MiB\")\n",
        "print(\"=== Оценка на тесте ===\")\n",
        "print(f\"Accuracy = {test_acc:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e28b5ade",
      "metadata": {
        "id": "e28b5ade"
      },
      "source": [
        "## Улучшенная LRCN с подобранными гиперпараметрами"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51496720",
      "metadata": {
        "id": "51496720"
      },
      "outputs": [],
      "source": [
        "def make_loader(X_arr, y_arr, batch_size=64, shuffle=False):\n",
        "  # Преобразование данных в тензоры для использования в модели\n",
        "    ds = TensorDataset(\n",
        "        torch.tensor(X_arr, dtype=torch.long),\n",
        "        torch.tensor(y_arr, dtype=torch.long)\n",
        "    )\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "# loader'ы для итераций по датасетам\n",
        "train_loader = make_loader(X_train, y_train, batch_size=64, shuffle=True)\n",
        "val_loader   = make_loader(X_val,   y_val,   batch_size=64, shuffle=False)\n",
        "test_loader  = make_loader(X_test,  y_test,  batch_size=64, shuffle=False)\n",
        "\n",
        "class ResidualCNNBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Остаточный блок на основе 1D-свёрток с пропускным соединением\n",
        "    channels: число каналов на входе и выходе\n",
        "    kernel_size: размер ядра свёртки\n",
        "    padding: паддинг по краям\n",
        "    \"\"\"\n",
        "    def __init__(self, channels, kernel_size=3, padding=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(channels, channels, kernel_size, padding=padding)\n",
        "        self.bn1 = nn.BatchNorm1d(channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv1d(channels, channels, kernel_size, padding=padding)\n",
        "        self.bn2 = nn.BatchNorm1d(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        return self.relu(out + x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Класс модели\n",
        "Наследуется от nn.Module (базовый класс для всех моделей в PyTorch)\n",
        "Улучшенная LRCN-модель:\n",
        "      - Эмбеддинги + Dropout\n",
        "      - Свёрточные слои + остаточные блоки\n",
        "      - Адаптивный пуллинг\n",
        "      - Двунаправленный LSTM\n",
        "      - Глобальный пулинг + Dropout + полносвязный слой\n",
        "\"\"\"\n",
        "class ImprovedLRCN(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_size, num_layers, num_classes,\n",
        "                 dropout_emb=0.1, dropout_lstm=0.1, dropout_fc=0.3):\n",
        "        super().__init__()\n",
        "        # Эмбеддинги + регуляризация\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        self.emb_dropout = nn.Dropout(dropout_emb)\n",
        "        # Входной свёрточный слой + нормализация\n",
        "        self.conv_in = nn.Conv1d(emb_dim, 128, kernel_size=3, padding=1)\n",
        "        self.bn_in = nn.BatchNorm1d(128)\n",
        "        self.res1 = ResidualCNNBlock(128)\n",
        "        self.pool = nn.AdaptiveMaxPool1d(output_size=seq_len//2)  # сохранить половину\n",
        "        self.conv_mid = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn_mid = nn.BatchNorm1d(256)\n",
        "        self.res2 = ResidualCNNBlock(256)\n",
        "        # Двунаправленный LSTM для захвата обоих направлений\n",
        "        self.lstm = nn.LSTM(input_size=256,\n",
        "                            hidden_size=hidden_size,\n",
        "                            num_layers=num_layers,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True,\n",
        "                            dropout=dropout_lstm if num_layers>1 else 0)\n",
        "        self.fc_dropout = nn.Dropout(dropout_fc)\n",
        "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        x = self.emb(x)\n",
        "        x = self.emb_dropout(x)\n",
        "        x = x.permute(0,2,1)\n",
        "        x = self.conv_in(x)\n",
        "        x = self.bn_in(x)\n",
        "        x = self.relu(x) if hasattr(self, 'relu') else nn.ReLU()(x)\n",
        "        x = self.res1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv_mid(x)\n",
        "        x = self.bn_mid(x)\n",
        "        x = self.res2(x)\n",
        "        x = x.permute(0,2,1)\n",
        "        out, _ = self.lstm(x)\n",
        "        context = torch.mean(out, dim=1)\n",
        "\n",
        "        h = self.fc_dropout(context)\n",
        "        return self.fc(h)\n",
        "\n",
        "# Функция цикла обучения модели\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer,\n",
        "                scheduler, num_epochs=30, clip_norm=5.0,\n",
        "                early_stopping_patience=5, device='cpu'):\n",
        "    model.to(device)\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        # Обучение по батчам\n",
        "        for Xb, yb in train_loader:\n",
        "            Xb, yb = Xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(Xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # валидация\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for Xb, yb in val_loader:\n",
        "                Xb, yb = Xb.to(device), yb.to(device)\n",
        "                logits = model(Xb)\n",
        "                val_loss += criterion(logits, yb).item()\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{num_epochs} — train_loss={avg_train_loss:.4f}, val_loss={avg_val_loss:.4f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_state = model.state_dict()\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= early_stopping_patience:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                model.load_state_dict(best_state)\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Оценка модели: возвращает accuracy, precision, recall и F1\n",
        "def evaluate_model(model, loader, device='cpu'):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in loader:\n",
        "            Xb = Xb.to(device)\n",
        "            logits = model(Xb)\n",
        "            preds = logits.argmax(dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(yb.numpy())\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    rec = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    f1  = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    return acc, prec, rec, f1"
      ],
      "metadata": {
        "id": "9XpmjzQMvF5o"
      },
      "id": "9XpmjzQMvF5o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3549bacc",
      "metadata": {
        "id": "3549bacc"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "vocab_size = int(X_train.max()) + 1\n",
        "num_classes = vocab_size\n",
        "seq_len = X_train.shape[1]\n",
        "\n",
        "# Используем CrossEntropy без весов или с мягкими весами\n",
        "crit = nn.CrossEntropyLoss()\n",
        "model = ImprovedLRCN(vocab_size, emb_dim=128, hidden_size=128,\n",
        "                     num_layers=2, num_classes=num_classes).to(device)\n",
        "opt = optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    opt, max_lr=3e-3,\n",
        "    steps_per_epoch=len(train_loader), epochs=20,\n",
        "    pct_start=0.1, anneal_strategy='cos'\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "# Замер текущей памяти\n",
        "process = psutil.Process(os.getpid())\n",
        "mem_before = process.memory_info().rss\n",
        "\n",
        "model = train_model(model, train_loader, val_loader, crit, opt, scheduler,\n",
        "                    num_epochs=20, clip_norm=5.0,\n",
        "                    early_stopping_patience=5, device=device)\n",
        "\n",
        "test_acc, precision, recall, f1 = evaluate_model(model, test_loader, device)\n",
        "print(f\"Время работы: {time.time()-start:.1f}s\")\n",
        "\n",
        "# Память после\n",
        "mem_after = process.memory_info().rss\n",
        "mem_used = mem_after - mem_before\n",
        "\n",
        "print(f\"Потребление памяти процесса: {mem_used / (1024**2):.2f} MiB\")\n",
        "\n",
        "print(f\"=== Оценка на тесте ===\")\n",
        "print(f\"Accuracy = {test_acc:.4f}\")\n",
        "print(f\"Precision = {precision:.4f}\")\n",
        "print(f\"Recall = {recall:.4f}\")\n",
        "print(f\"F1-Score = {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e49ff9ce",
      "metadata": {
        "id": "e49ff9ce"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd4afa1b",
      "metadata": {
        "id": "dd4afa1b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "963e6579",
      "metadata": {
        "id": "963e6579"
      },
      "outputs": [],
      "source": [
        "def make_loader(X_arr, y_arr, shuffle=False):\n",
        "    # Преобразование данных в тензоры для использования в модели\n",
        "    ds = TensorDataset(\n",
        "        torch.tensor(X_arr, dtype=torch.long),\n",
        "        torch.tensor(y_arr, dtype=torch.long)\n",
        "    )\n",
        "    return DataLoader(ds, batch_size=64, shuffle=shuffle)\n",
        "\n",
        "# loader'ы для итераций по датасетам\n",
        "train_loader = make_loader(X_train, y_train, shuffle=True)\n",
        "val_loader   = make_loader(X_val,   y_val,   shuffle=False)\n",
        "test_loader  = make_loader(X_test,  y_test,  shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f818b6e",
      "metadata": {
        "id": "8f818b6e"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Класс модели\n",
        "Наследуется от nn.Module (базовый класс для всех моделей в PyTorch)\n",
        "\"\"\"\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_size, num_layers, seq_length, pad_idx=0, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=emb_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0.2,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.layernorm = nn.LayerNorm(hidden_size * 2)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size * 2, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.emb(x)\n",
        "        out, _ = self.lstm(emb)\n",
        "        h = out[:, -1, :]\n",
        "        h = self.layernorm(h)\n",
        "        h = self.dropout(h)\n",
        "        return self.fc(h)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a5751cc",
      "metadata": {
        "id": "2a5751cc"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "vocab_size = int(X.max()) + 1\n",
        "\n",
        "# Замер текущей памяти\n",
        "process = psutil.Process(os.getpid())\n",
        "mem_before = process.memory_info().rss\n",
        "\n",
        "lstm_model = LSTM(\n",
        "    vocab_size=vocab_size,\n",
        "    emb_dim=128,\n",
        "    hidden_size=128,\n",
        "    num_layers=2,\n",
        "    seq_length=seq_length,\n",
        "    pad_idx=0,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5595d018",
      "metadata": {
        "id": "5595d018"
      },
      "outputs": [],
      "source": [
        "# Тренировочный цикл с валидацией по Accuracy\n",
        "best_val_acc = 0.0\n",
        "start = time.time()\n",
        "\n",
        "for epoch in range(1, 21):\n",
        "    lstm_model.train()\n",
        "    for Xb, yb in train_loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = lstm_model(Xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "    # Валидация\n",
        "    lstm_model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in val_loader:\n",
        "            Xb = Xb.to(device)\n",
        "            preds = lstm_model(Xb).argmax(dim=1).cpu().numpy()\n",
        "            all_preds.append(preds)\n",
        "            all_labels.append(yb.numpy())\n",
        "    val_preds = np.concatenate(all_preds)\n",
        "    val_labels = np.concatenate(all_labels)\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch}/20 — val_accuracy={val_acc:.4f}\")\n",
        "\n",
        "    # Сохраняем лучшую модель\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(lstm_model.state_dict(), 'best_lstm.pt')\n",
        "\n",
        "print(f\"Время работы: {time.time()-start:.1f}s\")\n",
        "\n",
        "# Память после\n",
        "mem_after = process.memory_info().rss\n",
        "mem_used = mem_after - mem_before\n",
        "\n",
        "print(f\"Потребление памяти процесса: {mem_used / (1024**2):.2f} MiB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5253ecf",
      "metadata": {
        "id": "b5253ecf"
      },
      "outputs": [],
      "source": [
        "# Оценка на тесте\n",
        "lstm_model.load_state_dict(torch.load('best_lstm.pt'))\n",
        "lstm_model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for Xb, yb in test_loader:\n",
        "        Xb = Xb.to(device)\n",
        "        preds = lstm_model(Xb).argmax(dim=1).cpu().numpy()\n",
        "        all_preds.append(preds)\n",
        "        all_labels.append(yb.numpy())\n",
        "\n",
        "test_preds = np.concatenate(all_preds)\n",
        "test_labels = np.concatenate(all_labels)\n",
        "test_acc = accuracy_score(test_labels, test_preds)\n",
        "precision = precision_score(test_labels, test_preds, average='weighted')\n",
        "recall = recall_score(test_labels, test_preds, average='weighted')\n",
        "f1 = f1_score(test_labels, test_preds, average='weighted')\n",
        "\n",
        "print(\"=== Оценка на тесте ===\")\n",
        "print(f\"Accuracy = {test_acc:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d96a7a26",
      "metadata": {
        "id": "d96a7a26"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "305524b6",
      "metadata": {
        "id": "305524b6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c961148",
      "metadata": {
        "id": "2c961148"
      },
      "outputs": [],
      "source": [
        "session_seqs = (\n",
        "    ddf\n",
        "      .groupby('session_id')['category_encoded']\n",
        "      .apply(lambda s: s.tolist(), meta=('list', object))\n",
        "      .compute()\n",
        ")\n",
        "# Оставляем только сессии длиной >= 10\n",
        "session_seqs = [seq for seq in session_seqs if len(seq) >= 10]\n",
        "\n",
        "# Генерация префиксов -> метки (следующая категория)\n",
        "sequences, labels = [], []\n",
        "for seq in session_seqs:\n",
        "    for i in range(1, len(seq)):\n",
        "        sequences.append(seq[:i])\n",
        "        labels.append(seq[i])\n",
        "\n",
        "# Усечение до фиксированной длины\n",
        "def pad_and_shift(seqs, seq_length):\n",
        "    X = np.zeros((len(seqs), seq_length), dtype=np.int64)\n",
        "    for i, seq in enumerate(seqs):\n",
        "        arr = np.array(seq, dtype=np.int64) + 1\n",
        "        if len(arr) >= seq_length:\n",
        "            X[i] = arr[-seq_length:]\n",
        "        else:\n",
        "            X[i, -len(arr):] = arr\n",
        "    return X\n",
        "\n",
        "seq_length = 10\n",
        "X = pad_and_shift(sequences, seq_length)\n",
        "y = np.array(labels, dtype=np.int64) + 1\n",
        "\n",
        "# Фильтрация редких меток (<2 возникновений)\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "valid = unique[counts >= 2]\n",
        "mask = np.isin(y, valid)\n",
        "X, y = X[mask], y[mask]\n",
        "\n",
        "print(f\"После фильтрации: всего сэмплов = {len(y)}, \"\n",
        "      f\"уникальных классов = {len(np.unique(y))}\")\n",
        "\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.20,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val,\n",
        "    test_size=0.25,\n",
        "    random_state=42,\n",
        "    stratify=y_train_val\n",
        ")\n",
        "print(f\"Размер X_train: {X_train.shape}\")\n",
        "print(f\"Размер X_test:  {X_test.shape}\")\n",
        "print(f\"Размер X_val:  {X_val.shape}\")\n",
        "print(f\"Размер y_train: {y_train.shape}\")\n",
        "print(f\"Размер y_test: {y_test.shape}\")\n",
        "print(f\"Размер y_val:  {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3580b3f8",
      "metadata": {
        "id": "3580b3f8"
      },
      "outputs": [],
      "source": [
        "def make_loader(X_arr, y_arr, shuffle=False):\n",
        "    # Преобразование данных в тензоры для использования в модели\n",
        "    ds = TensorDataset(\n",
        "        torch.tensor(X_arr, dtype=torch.long),\n",
        "        torch.tensor(y_arr, dtype=torch.long)\n",
        "    )\n",
        "    return DataLoader(ds, batch_size=64, shuffle=shuffle)\n",
        "\n",
        "# loader'ы для итераций по датасетам\n",
        "train_loader = make_loader(X_train, y_train, shuffle=True)\n",
        "val_loader   = make_loader(X_val,   y_val,   shuffle=False)\n",
        "test_loader  = make_loader(X_test,  y_test,  shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aa680d3",
      "metadata": {
        "id": "0aa680d3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Добавляет к входным эмбеддингам информацию о позиции токенов в последовательности.\n",
        "\"\"\"\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=500):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b41b3fb",
      "metadata": {
        "id": "7b41b3fb"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Класс модели\n",
        "Наследуется от nn.Module (базовый класс для всех моделей в PyTorch)\n",
        "\"\"\"\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 d_model: int = 128,\n",
        "                 nhead: int = 4,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 num_layers: int = 2,\n",
        "                 seq_length: int = 5,\n",
        "                 pad_idx: int = 0,\n",
        "                 dropout: float = 0.3):\n",
        "        super().__init__()\n",
        "        # слой эмбеддинга\n",
        "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        # позиционное кодирование\n",
        "        self.pos_enc = PositionalEncoding(d_model, max_len=seq_length)\n",
        "        # формируем один слой энкодера Transformer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        pad_mask = (x == self.emb.padding_idx)\n",
        "        e = self.emb(x)\n",
        "        e = self.pos_enc(e)\n",
        "        out = self.transformer(e, src_key_padding_mask=pad_mask)\n",
        "        h = out[:, -1, :]\n",
        "        h = self.layernorm(h)\n",
        "        h = self.dropout(h)\n",
        "        return self.fc(h)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29196823",
      "metadata": {
        "id": "29196823"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "vocab_size = int(X.max()) + 1\n",
        "\n",
        "# Замер текущей памяти\n",
        "process = psutil.Process(os.getpid())\n",
        "mem_before = process.memory_info().rss\n",
        "\n",
        "tr_model = Transformer(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=128,\n",
        "    nhead=4,\n",
        "    dim_feedforward=512,\n",
        "    num_layers=2,\n",
        "    seq_length=seq_length,\n",
        "    pad_idx=0,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(tr_model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cbf3f4c",
      "metadata": {
        "id": "3cbf3f4c"
      },
      "outputs": [],
      "source": [
        "# Тренировочный цикл с валидацией по Accuracy\n",
        "best_val_acc = 0.0\n",
        "start = time.time()\n",
        "\n",
        "for epoch in range(1, 21):\n",
        "    tr_model.train()\n",
        "    for Xb, yb in train_loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = tr_model(Xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(tr_model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "    # Валидация\n",
        "    tr_model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in val_loader:\n",
        "            Xb = Xb.to(device)\n",
        "            preds = tr_model(Xb).argmax(dim=1).cpu().numpy()\n",
        "            all_preds.append(preds)\n",
        "            all_labels.append(yb.numpy())\n",
        "    val_preds = np.concatenate(all_preds)\n",
        "    val_labels = np.concatenate(all_labels)\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch}/20 — val_accuracy={val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(tr_model.state_dict(), 'best_transformer.pt')\n",
        "\n",
        "print(f\"Время работы: {time.time()-start:.1f}s\")\n",
        "\n",
        "# Память после\n",
        "mem_after = process.memory_info().rss\n",
        "mem_used = mem_after - mem_before\n",
        "\n",
        "print(f\"Потребление памяти процесса: {mem_used / (1024**2):.2f} MiB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce3d0757",
      "metadata": {
        "id": "ce3d0757"
      },
      "outputs": [],
      "source": [
        "# Оценка на тесте\n",
        "tr_model.load_state_dict(torch.load('best_transformer.pt'))\n",
        "tr_model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for Xb, yb in test_loader:\n",
        "        Xb = Xb.to(device)\n",
        "        preds = tr_model(Xb).argmax(dim=1).cpu().numpy()\n",
        "        all_preds.append(preds)\n",
        "        all_labels.append(yb.numpy())\n",
        "\n",
        "test_preds = np.concatenate(all_preds)\n",
        "test_labels = np.concatenate(all_labels)\n",
        "test_acc = accuracy_score(test_labels, test_preds)\n",
        "precision = precision_score(test_labels,\n",
        "                            test_preds,\n",
        "                            average='weighted',\n",
        "                            zero_division=0)\n",
        "recall = recall_score(test_labels,\n",
        "                         test_preds,\n",
        "                         average='weighted',\n",
        "                         zero_division=0)\n",
        "f1 = f1_score(test_labels,\n",
        "                      test_preds,\n",
        "                      average='weighted',\n",
        "                      zero_division=0)\n",
        "\n",
        "print(\"=== Оценка на тесте ===\")\n",
        "print(f\"Accuracy = {test_acc:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73fa5622",
      "metadata": {
        "id": "73fa5622"
      },
      "source": [
        "# FFNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dfbb6d7",
      "metadata": {
        "id": "6dfbb6d7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bac789d2",
      "metadata": {
        "id": "bac789d2"
      },
      "outputs": [],
      "source": [
        "seq_length = 5\n",
        "\n",
        "merged_data = merged_data.sort_values(['session_id', 'timestamp'])\n",
        "\n",
        "ddf = dd.from_pandas(\n",
        "    merged_data[['session_id','category_encoded']],\n",
        "    npartitions=16\n",
        ")\n",
        "session_seqs = (\n",
        "    ddf\n",
        "      .groupby('session_id')['category_encoded']\n",
        "      .apply(lambda s: s.tolist(), meta=('list', object))\n",
        "      .compute()\n",
        ")\n",
        "\n",
        "# Оставляем только сессии длиной >= 5\n",
        "session_seqs = [seq for seq in session_seqs if len(seq) >= 5]\n",
        "\n",
        "# Генерируем префиксы → метки\n",
        "sequences, labels = [], []\n",
        "for seq in session_seqs:\n",
        "    for i in range(1, len(seq)):\n",
        "        sequences.append(seq[:i])\n",
        "        labels.append(seq[i])\n",
        "\n",
        "# Паддинг/выравнивание вправо + сдвиг всех категорий на +1\n",
        "def pad_and_shift(seqs, seq_length):\n",
        "    X = np.zeros((len(seqs), seq_length), dtype=np.int64)  # 0 = padding\n",
        "    for i, seq in enumerate(seqs):\n",
        "        arr = np.array(seq, dtype=np.int64) + 1  # реальные категории\n",
        "        if len(arr) >= seq_length:\n",
        "            X[i] = arr[-seq_length:]\n",
        "        else:\n",
        "            X[i, -len(arr):] = arr\n",
        "    return X\n",
        "\n",
        "X = pad_and_shift(sequences, 5)\n",
        "y = np.array(labels, dtype=np.int64) + 1   # метки тоже +1\n",
        "\n",
        "# Фильтруем очень редкие метки\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "valid = unique[counts >= 2]\n",
        "mask = np.isin(y, valid)\n",
        "X, y = X[mask], y[mask]\n",
        "\n",
        "print(f\"После фильтрации: всего сэмплов = {len(y)}, \"\n",
        "      f\"уникальных классов = {len(valid)}\")\n",
        "\n",
        "# Разделяем на train/val/test\n",
        "X_trval, X_test, y_trval, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42, stratify=y\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_trval, y_trval, test_size=0.25, random_state=42, stratify=y_trval\n",
        ")\n",
        "print(f\"Размер X_train: {X_train.shape}\")\n",
        "print(f\"Размер X_test:  {X_test.shape}\")\n",
        "print(f\"Размер X_val:  {X_val.shape}\")\n",
        "print(f\"Размер y_train: {y_train.shape}\")\n",
        "print(f\"Размер y_test: {y_test.shape}\")\n",
        "print(f\"Размер y_val:  {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e621c876",
      "metadata": {
        "id": "e621c876"
      },
      "outputs": [],
      "source": [
        "# Вычисляем веса для классов из train\n",
        "class_counts = np.bincount(y_train)\n",
        "class_weights = np.zeros_like(class_counts, dtype=np.float32)\n",
        "nonzero = class_counts > 0\n",
        "class_weights[nonzero] = 1.0 / class_counts[nonzero]\n",
        "\n",
        "# Для CrossEntropyLoss\n",
        "weight_tensor = torch.from_numpy(class_weights).float()\n",
        "\n",
        "# Для сэмплера\n",
        "sample_weights = class_weights[y_train]\n",
        "sampler = WeightedRandomSampler(sample_weights,\n",
        "                                num_samples=len(sample_weights),\n",
        "                                replacement=True)\n",
        "\n",
        "def make_loader(X_arr, y_arr, sampler=None, shuffle=False):\n",
        "    # Преобразование данных в тензоры для использования в модели\n",
        "    ds = TensorDataset(torch.tensor(X_arr, dtype=torch.long),\n",
        "                       torch.tensor(y_arr, dtype=torch.long))\n",
        "    return DataLoader(ds,\n",
        "                      batch_size=64,\n",
        "                      sampler=sampler,\n",
        "                      shuffle=shuffle and sampler is None)\n",
        "\n",
        "# loader'ы для итераций по датасетам\n",
        "train_loader = make_loader(X_train, y_train, sampler=sampler)\n",
        "val_loader   = make_loader(X_val,   y_val,   shuffle=False)\n",
        "test_loader  = make_loader(X_test,  y_test,  shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acd060a4",
      "metadata": {
        "id": "acd060a4"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Класс модели\n",
        "Наследуется от nn.Module (базовый класс для всех моделей в PyTorch)\n",
        "\"\"\"\n",
        "\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, seq_length, hidden_dim, dropout, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.emb  = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.fc1  = nn.Linear(emb_dim * seq_length, hidden_dim)\n",
        "        self.bn1  = nn.BatchNorm1d(hidden_dim)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.do   = nn.Dropout(dropout)\n",
        "        self.fc2  = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e    = self.emb(x)\n",
        "        flat = e.view(e.size(0), -1)\n",
        "        h    = self.fc1(flat)\n",
        "        h    = self.bn1(h)\n",
        "        h    = self.relu(h)\n",
        "        h    = self.do(h)\n",
        "        return self.fc2(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc779fbf",
      "metadata": {
        "id": "cc779fbf"
      },
      "outputs": [],
      "source": [
        "device     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "vocab_size = int(X.max()) + 1\n",
        "\n",
        "# Замер текущей памяти\n",
        "process = psutil.Process(os.getpid())\n",
        "mem_before = process.memory_info().rss\n",
        "\n",
        "ffnn_model = FFNN(vocab_size, 128, 5, 256, 0.3).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=weight_tensor.to(device))\n",
        "optimizer = optim.Adam(ffnn_model.parameters(), lr=1e-3)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=1e-3 * 10,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccb6eca0",
      "metadata": {
        "id": "ccb6eca0"
      },
      "outputs": [],
      "source": [
        "# Тренировочный цикл с валидацией по Accuracy\n",
        "best_val_acc = 0.0\n",
        "start = time.time()\n",
        "\n",
        "for epoch in range(1, 21):\n",
        "    ffnn_model.train()\n",
        "    for Xb, yb in train_loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = ffnn_model(Xb)\n",
        "        loss   = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    ffnn_model.eval()\n",
        "    preds, labs = [], []\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in val_loader:\n",
        "            Xb = Xb.to(device)\n",
        "            out = ffnn_model(Xb).argmax(dim=1).cpu().numpy()\n",
        "            preds.append(out)\n",
        "            labs.append(yb.numpy())\n",
        "    val_preds  = np.concatenate(preds)\n",
        "    val_labels = np.concatenate(labs)\n",
        "    val_acc    = accuracy_score(val_labels, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch}/{20} — val_acc={val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(ffnn_model.state_dict(), 'best_ffnn.pt')\n",
        "\n",
        "print(f\"Время работы: {time.time()-start:.1f}s\")\n",
        "\n",
        "# Память после\n",
        "mem_after = process.memory_info().rss\n",
        "mem_used = mem_after - mem_before\n",
        "\n",
        "print(f\"Потребление памяти процесса: {mem_used / (1024**2):.2f} MiB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a4b6d65",
      "metadata": {
        "id": "3a4b6d65"
      },
      "outputs": [],
      "source": [
        "# Оценка на тесте\n",
        "ffnn_model.load_state_dict(torch.load('best_ffnn.pt'))\n",
        "ffnn_model.eval()\n",
        "preds, labs = [], []\n",
        "with torch.no_grad():\n",
        "    for Xb, yb in test_loader:\n",
        "        Xb = Xb.to(device)\n",
        "        out = ffnn_model(Xb).argmax(dim=1).cpu().numpy()\n",
        "        preds.append(out)\n",
        "        labs.append(yb.numpy())\n",
        "\n",
        "test_preds  = np.concatenate(preds)\n",
        "test_labels = np.concatenate(labs)\n",
        "test_acc    = accuracy_score(test_labels, test_preds)\n",
        "precision = precision_score(test_labels,\n",
        "                            test_preds,\n",
        "                            average='weighted',\n",
        "                            zero_division=0)\n",
        "recall = recall_score(test_labels,\n",
        "                         test_preds,\n",
        "                         average='weighted',\n",
        "                         zero_division=0)\n",
        "f1 = f1_score(test_labels,\n",
        "                      test_preds,\n",
        "                      average='weighted',\n",
        "                      zero_division=0)\n",
        "\n",
        "print(\"=== Оценка на тесте ===\")\n",
        "print(f\"Accuracy = {test_acc:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "224c7ff0",
      "metadata": {
        "id": "224c7ff0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "prec_macro = precision_score(test_labels, test_preds, average='macro', zero_division=0)\n",
        "rec_macro  = recall_score(   test_labels, test_preds, average='macro', zero_division=0)\n",
        "print(f\"Macro Precision: {prec_macro:.4f}, Macro Recall: {rec_macro:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}